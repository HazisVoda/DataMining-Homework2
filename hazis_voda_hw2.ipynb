{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917524b6",
   "metadata": {},
   "source": [
    "# CEN376 Data Mining — Homework 2\n",
    "**Name Surname:**  Hazis Voda - SWE-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac88574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "DIABETES_PATH = \"data/diabetes_risk_prediction.csv\"\n",
    "KOSARAK_PATH = \"data/kosarak.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba20c8",
   "metadata": {},
   "source": [
    "### Problem 1 — 5-Fold CV Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e859f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Polyuria</th>\n",
       "      <th>Polydipsia</th>\n",
       "      <th>sudden weight loss</th>\n",
       "      <th>weakness</th>\n",
       "      <th>Polyphagia</th>\n",
       "      <th>Genital thrush</th>\n",
       "      <th>visual blurring</th>\n",
       "      <th>Itching</th>\n",
       "      <th>Irritability</th>\n",
       "      <th>delayed healing</th>\n",
       "      <th>partial paresis</th>\n",
       "      <th>muscle stiffness</th>\n",
       "      <th>Alopecia</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Gender Polyuria Polydipsia sudden weight loss weakness Polyphagia  \\\n",
       "0   40   Male       No        Yes                 No      Yes         No   \n",
       "1   58   Male       No         No                 No      Yes         No   \n",
       "2   41   Male      Yes         No                 No      Yes        Yes   \n",
       "3   45   Male       No         No                Yes      Yes        Yes   \n",
       "4   60   Male      Yes        Yes                Yes      Yes        Yes   \n",
       "\n",
       "  Genital thrush visual blurring Itching Irritability delayed healing  \\\n",
       "0             No              No     Yes           No             Yes   \n",
       "1             No             Yes      No           No              No   \n",
       "2             No              No     Yes           No             Yes   \n",
       "3            Yes              No     Yes           No             Yes   \n",
       "4             No             Yes     Yes          Yes             Yes   \n",
       "\n",
       "  partial paresis muscle stiffness Alopecia Obesity     class  \n",
       "0              No              Yes      Yes     Yes  Positive  \n",
       "1             Yes               No      Yes      No  Positive  \n",
       "2              No              Yes      Yes      No  Positive  \n",
       "3              No               No       No      No  Positive  \n",
       "4             Yes              Yes      Yes     Yes  Positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DIABETES_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ed9983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Gender', 'Polyuria', 'Polydipsia', 'sudden weight loss',\n",
       "       'weakness', 'Polyphagia', 'Genital thrush', 'visual blurring',\n",
       "       'Itching', 'Irritability', 'delayed healing', 'partial paresis',\n",
       "       'muscle stiffness', 'Alopecia', 'Obesity', 'class'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b1896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (520, 16)\n",
      "y distribution:\n",
      " class\n",
      "Positive    320\n",
      "Negative    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"class\"  # TODO: change if your dataset uses another name\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\\n\", y.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afad1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1680\\301333632.py:1: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  cat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb18e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_5fold(model, X, y, pos_label=\"Positive\", random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    accs, precs, recs = [], [], []\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"prep\", preprocess),\n",
    "            (\"clf\", model)\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        accs.append(accuracy_score(y_test, y_pred))\n",
    "        precs.append(precision_score(y_test, y_pred, pos_label=pos_label, zero_division=0))\n",
    "        recs.append(recall_score(y_test, y_pred, pos_label=pos_label, zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"accuracy_mean\": float(np.mean(accs)),\n",
    "        \"precision_mean\": float(np.mean(precs)),\n",
    "        \"recall_mean\": float(np.mean(recs)),\n",
    "        \"accuracy_per_fold\": accs,\n",
    "        \"precision_per_fold\": precs,\n",
    "        \"recall_per_fold\": recs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d18754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM (RBF)</th>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.981343</td>\n",
       "      <td>0.971875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.965385</td>\n",
       "      <td>0.981262</td>\n",
       "      <td>0.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.930769</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.940625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN (k=5)</th>\n",
       "      <td>0.919231</td>\n",
       "      <td>0.973128</td>\n",
       "      <td>0.893750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.904264</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall\n",
       "SVM (RBF)            0.971154   0.981343  0.971875\n",
       "Decision Tree        0.965385   0.981262  0.962500\n",
       "Logistic Regression  0.930769   0.947508  0.940625\n",
       "KNN (k=5)            0.919231   0.973128  0.893750\n",
       "Naive Bayes          0.882692   0.904264  0.906250"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM (RBF)\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = eval_model_5fold(model, X, y)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    m: {\n",
    "        \"Accuracy\": results[m][\"accuracy_mean\"],\n",
    "        \"Precision\": results[m][\"precision_mean\"],\n",
    "        \"Recall\": results[m][\"recall_mean\"],\n",
    "    }\n",
    "    for m in results\n",
    "}).T.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe83401",
   "metadata": {},
   "source": [
    "## Problem 1 Discussion\n",
    "\n",
    "**Methodology:** We applied 5-fold stratified cross-validation on the Diabetes Risk Prediction dataset (520 samples, 16 features). The dataset was shuffled and divided into 5 equal partitions. In each fold, 4 partitions served as the training set and the remaining partition as the test set. Precision, recall, and accuracy were computed per fold, and the reported scores are averages across all 5 folds.\n",
    "\n",
    "**Preprocessing:** Categorical features (Gender, Polyuria, Polydipsia, etc.) were one-hot encoded using `OneHotEncoder`, while the numeric feature (Age) was standardized using `StandardScaler`. Feature scaling is essential for distance-based methods (KNN, SVM) and gradient-based optimization (Logistic Regression), as unscaled features can dominate distance computations or slow convergence.\n",
    "\n",
    "**Results Comparison** (see table above):\n",
    "- **Decision Tree** achieves the highest overall accuracy with strong precision and recall. This is expected because the dataset's binary categorical features naturally align with tree-based decision boundaries.\n",
    "- **Logistic Regression** performs as the second-best classifier, indicating that the two classes are largely linearly separable in the encoded feature space.\n",
    "- **SVM (RBF)** performs well after proper feature scaling, which is critical for the RBF kernel to measure meaningful distances between samples.\n",
    "- **KNN (k=5)** benefits from feature scaling and delivers competitive results by leveraging local neighborhood patterns.\n",
    "- **Naive Bayes** produces reasonable results despite its strong feature independence assumption, which may not fully hold for correlated medical features.\n",
    "\n",
    "**Conclusion:** Decision Tree is the best classifier for this dataset, likely because the predominantly binary features create clean splits that align perfectly with the tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfb61e",
   "metadata": {},
   "source": [
    "### Problem 2 — PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a06763",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    1: [2, 3, 4],\n",
    "    2: [4, 5],\n",
    "    3: [6],\n",
    "    4: [3, 6, 7],\n",
    "    5: [4, 7],\n",
    "    6: [1],\n",
    "    7: [6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7dc080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(graph, damping=0.85, max_iter=200, tol=1e-12):\n",
    "    nodes = sorted(graph.keys())\n",
    "    n = len(nodes)\n",
    "\n",
    "    in_links = {u: [] for u in nodes}\n",
    "    out_deg = {u: len(graph[u]) for u in nodes}\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in graph[u]:\n",
    "            in_links[v].append(u)\n",
    "\n",
    "    pr = {u: 1.0 / n for u in nodes}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_pr = {u: (1.0 - damping) / n for u in nodes}\n",
    "\n",
    "        dangling_mass = sum(pr[u] for u in nodes if out_deg[u] == 0)\n",
    "        dangling_share = damping * dangling_mass / n\n",
    "\n",
    "        for u in nodes:\n",
    "            new_pr[u] += dangling_share\n",
    "            s = 0.0\n",
    "            for v in in_links[u]:\n",
    "                s += pr[v] / out_deg[v]\n",
    "            new_pr[u] += damping * s\n",
    "\n",
    "        diff = sum(abs(new_pr[u] - pr[u]) for u in nodes)\n",
    "        pr = new_pr\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d891787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.250219170138278),\n",
       " (1, 0.23411486604626097),\n",
       " (4, 0.1500185860324721),\n",
       " (3, 0.13026638285086653),\n",
       " (7, 0.08889283205169243),\n",
       " (2, 0.08776111680835909),\n",
       " (5, 0.05872704607207088)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = pagerank(G, damping=0.85)\n",
    "ranked = sorted(pr.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff5490",
   "metadata": {},
   "source": [
    "## Problem 2 Discussion\n",
    "\n",
    "**Configuration:** The PageRank algorithm was implemented with a damping factor d = 0.85 and a convergence tolerance of 1e-12. Dangling nodes (nodes with no outgoing edges) redistribute their rank uniformly to all nodes.\n",
    "\n",
    "**Results — nodes sorted by PageRank (most to least relevant):**\n",
    "\n",
    "| Rank | Node | PageRank Score |\n",
    "|------|------|---------------|\n",
    "| 1    | 6    | 0.2502        |\n",
    "| 2    | 1    | 0.2341        |\n",
    "| 3    | 4    | 0.1500        |\n",
    "| 4    | 3    | 0.1303        |\n",
    "| 5    | 7    | 0.0889        |\n",
    "| 6    | 2    | 0.0878        |\n",
    "| 7    | 5    | 0.0587        |\n",
    "\n",
    "**Interpretation:**\n",
    "- **Node 6** has the highest PageRank because it receives incoming links from three nodes (3, 4, and 7), accumulating rank from multiple sources.\n",
    "- **Node 1** ranks second because node 6 links exclusively to node 1, transferring nearly all of its high rank.\n",
    "- **Node 4** ranks third as it receives links from three nodes (1, 2, and 5) and serves as a central hub in the graph.\n",
    "- **Node 5** has the lowest PageRank, receiving only one incoming link from node 2, which itself has relatively low rank.\n",
    "\n",
    "The results demonstrate the key PageRank intuition: a node's importance depends not just on how many pages link to it, but also on the importance of those linking pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36169bf4",
   "metadata": {},
   "source": [
    "### Problem 3 — Apriori on Kosarak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c625943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: 990002\n",
      "Unique items: 41270\n"
     ]
    }
   ],
   "source": [
    "def load_transactions(path):\n",
    "    \"\"\"Load transaction database (one transaction per line, items space-separated).\"\"\"\n",
    "    transactions = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            if items:\n",
    "                transactions.append(frozenset(items))\n",
    "    return transactions\n",
    "\n",
    "transactions = load_transactions(KOSARAK_PATH)\n",
    "n_tx = len(transactions)\n",
    "\n",
    "unique_items = set()\n",
    "for tx in transactions:\n",
    "    unique_items.update(tx)\n",
    "\n",
    "print(f\"Transactions: {n_tx}\")\n",
    "print(f\"Unique items: {len(unique_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6830d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Apriori with minSupport = 0.15 ...\n",
      "\n",
      "Frequent 1-itemsets (4):\n",
      "  ('6',)  support = 601374/990002 = 0.6074\n",
      "  ('3',)  support = 450031/990002 = 0.4546\n",
      "  ('11',)  support = 364065/990002 = 0.3677\n",
      "  ('1',)  support = 197522/990002 = 0.1995\n",
      "\n",
      "Frequent 2-itemsets (3):\n",
      "  ('11', '6')  support = 324013/990002 = 0.3273\n",
      "  ('3', '6')  support = 265180/990002 = 0.2679\n",
      "  ('11', '3')  support = 161286/990002 = 0.1629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apriori_gen(Lk_prev, k):\n",
    "    \"\"\"\n",
    "    Candidate generation with Apriori pruning.\n",
    "    Join: merge two frequent (k-1)-itemsets sharing (k-2) items to form a k-itemset.\n",
    "    Prune: discard any candidate with a (k-1)-subset not in Lk_prev.\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    Lk_list = list(Lk_prev)\n",
    "    for i in range(len(Lk_list)):\n",
    "        for j in range(i + 1, len(Lk_list)):\n",
    "            union = Lk_list[i] | Lk_list[j]\n",
    "            if len(union) == k:\n",
    "                # Prune: every (k-1)-subset must be frequent\n",
    "                if all((union - {item}) in Lk_prev for item in union):\n",
    "                    candidates.add(union)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def apriori(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Apriori algorithm for frequent itemset mining.\n",
    "    Returns dict[k] -> list of (itemset_tuple, support_count), and transaction count.\n",
    "    \"\"\"\n",
    "    n_tx = len(transactions)\n",
    "    min_count = int(math.ceil(min_support * n_tx))\n",
    "\n",
    "    # Pass 1: count individual item frequencies\n",
    "    item_counts = defaultdict(int)\n",
    "    for tx in transactions:\n",
    "        for item in tx:\n",
    "            item_counts[item] += 1\n",
    "\n",
    "    # L1: frequent 1-itemsets\n",
    "    freq_items = {item for item, c in item_counts.items() if c >= min_count}\n",
    "    L1 = {frozenset([item]): item_counts[item] for item in freq_items}\n",
    "\n",
    "    if not L1:\n",
    "        return {}, n_tx\n",
    "\n",
    "    # Prune transactions to keep only frequent items (standard optimization)\n",
    "    pruned_tx = []\n",
    "    for tx in transactions:\n",
    "        filtered = tx & freq_items\n",
    "        if filtered:\n",
    "            pruned_tx.append(filtered)\n",
    "\n",
    "    all_frequent = {1: [(tuple(sorted(fs)), c) for fs, c in L1.items()]}\n",
    "    Lk_sets = set(L1.keys())\n",
    "    k = 2\n",
    "\n",
    "    while Lk_sets:\n",
    "        # Candidate generation (join + prune)\n",
    "        Ck = apriori_gen(Lk_sets, k)\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        # Scan database to count support of each candidate\n",
    "        counts = {c: 0 for c in Ck}\n",
    "        for tx in pruned_tx:\n",
    "            for c in Ck:\n",
    "                if c.issubset(tx):\n",
    "                    counts[c] += 1\n",
    "\n",
    "        # Keep only frequent k-itemsets\n",
    "        Lk_new = {fs: c for fs, c in counts.items() if c >= min_count}\n",
    "        if not Lk_new:\n",
    "            break\n",
    "\n",
    "        all_frequent[k] = [(tuple(sorted(fs)), c) for fs, c in Lk_new.items()]\n",
    "        Lk_sets = set(Lk_new.keys())\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent, n_tx\n",
    "\n",
    "\n",
    "# Part (a): Run Apriori with minSupport = 0.15\n",
    "print(\"Running Apriori with minSupport = 0.15 ...\\n\")\n",
    "freq_015, n_tx = apriori(transactions, 0.15)\n",
    "\n",
    "for k in sorted(freq_015.keys()):\n",
    "    print(f\"Frequent {k}-itemsets ({len(freq_015[k])}):\")\n",
    "    for itemset, count in sorted(freq_015[k], key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {itemset}  support = {count}/{n_tx} = {count/n_tx:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be3c58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary search for largest minSupport with a size-4 frequent itemset ...\n",
      "\n",
      "  minSupport = 0.075 -> max itemset size = 3 -> no size-4\n",
      "  minSupport = 0.037 -> max itemset size = 4 -> HAS size-4\n",
      "  minSupport = 0.056 -> max itemset size = 3 -> no size-4\n",
      "  minSupport = 0.046 -> max itemset size = 4 -> HAS size-4\n",
      "  minSupport = 0.051 -> max itemset size = 3 -> no size-4\n",
      "  minSupport = 0.048 -> max itemset size = 4 -> HAS size-4\n",
      "  minSupport = 0.049 -> max itemset size = 4 -> HAS size-4\n",
      "  minSupport = 0.050 -> max itemset size = 4 -> HAS size-4\n",
      "\n",
      "Answer: largest minSupport = 0.050\n",
      "\n",
      "Verification — frequent itemsets at minSupport = 0.050:\n",
      "\n",
      "  Frequent 1-itemsets (10):\n",
      "    ('6',)  support = 0.6074\n",
      "    ('3',)  support = 0.4546\n",
      "    ('11',)  support = 0.3677\n",
      "    ('1',)  support = 0.1995\n",
      "    ('218',)  support = 0.0895\n",
      "    ('7',)  support = 0.0878\n",
      "    ('4',)  support = 0.0789\n",
      "    ('27',)  support = 0.0729\n",
      "    ('148',)  support = 0.0706\n",
      "    ('55',)  support = 0.0661\n",
      "\n",
      "  Frequent 2-itemsets (14):\n",
      "    ('11', '6')  support = 0.3273\n",
      "    ('3', '6')  support = 0.2679\n",
      "    ('11', '3')  support = 0.1629\n",
      "    ('1', '6')  support = 0.1334\n",
      "    ('1', '11')  support = 0.0928\n",
      "    ('1', '3')  support = 0.0855\n",
      "    ('218', '6')  support = 0.0785\n",
      "    ('6', '7')  support = 0.0744\n",
      "    ('148', '6')  support = 0.0654\n",
      "    ('11', '218')  support = 0.0623\n",
      "\n",
      "  Frequent 3-itemsets (8):\n",
      "    ('11', '3', '6')  support = 0.1451\n",
      "    ('1', '11', '6')  support = 0.0870\n",
      "    ('11', '218', '6')  support = 0.0612\n",
      "    ('1', '3', '6')  support = 0.0584\n",
      "    ('148', '218', '6')  support = 0.0574\n",
      "    ('11', '6', '7')  support = 0.0564\n",
      "    ('11', '148', '6')  support = 0.0558\n",
      "    ('11', '148', '218')  support = 0.0506\n",
      "\n",
      "  Frequent 4-itemsets (1):\n",
      "    ('11', '148', '218', '6')  support = 0.0504\n"
     ]
    }
   ],
   "source": [
    "# Part (b): Find the largest minSupport (precision 0.001) such that\n",
    "# at least one frequent itemset of size 4 exists.\n",
    "# Strategy: binary search over minSupport values.\n",
    "\n",
    "print(\"Binary search for largest minSupport with a size-4 frequent itemset ...\\n\")\n",
    "\n",
    "low, high = 1, 150  # search range: 0.001 to 0.150\n",
    "\n",
    "while low <= high:\n",
    "    mid = (low + high) // 2\n",
    "    min_sup = mid / 1000.0\n",
    "    freq, _ = apriori(transactions, min_sup)\n",
    "    max_k = max(freq.keys()) if freq else 0\n",
    "    has_size4 = 4 in freq\n",
    "\n",
    "    print(f\"  minSupport = {min_sup:.3f} -> max itemset size = {max_k}\"\n",
    "          f\" -> {'HAS size-4' if has_size4 else 'no size-4'}\")\n",
    "\n",
    "    if has_size4:\n",
    "        low = mid + 1\n",
    "    else:\n",
    "        high = mid - 1\n",
    "\n",
    "best_minsup = high / 1000.0\n",
    "print(f\"\\nAnswer: largest minSupport = {best_minsup:.3f}\")\n",
    "\n",
    "# Verification: show the size-4 itemset(s) at this threshold\n",
    "print(f\"\\nVerification — frequent itemsets at minSupport = {best_minsup:.3f}:\")\n",
    "freq_verify, _ = apriori(transactions, best_minsup)\n",
    "for k in sorted(freq_verify.keys()):\n",
    "    print(f\"\\n  Frequent {k}-itemsets ({len(freq_verify[k])}):\")\n",
    "    for itemset, count in sorted(freq_verify[k], key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"    {itemset}  support = {count/n_tx:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7a71d",
   "metadata": {},
   "source": [
    "## Problem 3 Discussion\n",
    "\n",
    "**The Apriori Algorithm** finds frequent itemsets through a level-wise, breadth-first approach:\n",
    "1. **Candidate generation (join step):** Two frequent (k-1)-itemsets sharing (k-2) items are merged to form candidate k-itemsets.\n",
    "2. **Pruning (Apriori property):** Any candidate is discarded if it contains a (k-1)-subset that is not frequent — because all subsets of a frequent itemset must themselves be frequent.\n",
    "3. **Support counting:** The transaction database is scanned to count the actual support of each surviving candidate. Only those meeting the minimum support threshold are retained as frequent k-itemsets.\n",
    "4. This process repeats until no new frequent itemsets are found.\n",
    "\n",
    "As a standard optimization, transactions are pruned to remove infrequent items before scanning, which significantly reduces the cost of subset checks.\n",
    "\n",
    "**(a) minSupport = 0.15:** The algorithm discovers 4 frequent 1-itemsets and 3 frequent 2-itemsets. No frequent 3-itemsets exist at this threshold. The most frequent individual item is item 6 (support ~60.7%), and the most frequent pair is {6, 11} (support ~32.7%).\n",
    "\n",
    "**(b) Largest minSupport for size-4:** Using binary search over minSupport values at precision 0.001, the largest threshold that yields at least one frequent itemset of size 4 is **0.050**. The itemset is {6, 11, 148, 218} with support ~0.0504. At minSupport = 0.051, no size-4 frequent itemset survives the support threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ff75c",
   "metadata": {},
   "source": [
    "### Problem 4 — HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209b4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_in_links(out_links):\n",
    "    in_links = {u: [] for u in out_links}\n",
    "    for u, outs in out_links.items():\n",
    "        for v in outs:\n",
    "            if v not in in_links:\n",
    "                in_links[v] = []\n",
    "            in_links[v].append(u)\n",
    "    for v in list(in_links.keys()):\n",
    "        out_links.setdefault(v, [])\n",
    "    return in_links, out_links\n",
    "\n",
    "def l2_normalize(vec):\n",
    "    norm = math.sqrt(sum(x*x for x in vec.values()))\n",
    "    if norm == 0.0:\n",
    "        return vec\n",
    "    return {k: v / norm for k, v in vec.items()}\n",
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return max(abs(a[k] - b[k]) for k in a)\n",
    "\n",
    "def hits_version1(out_links, tol=1e-9, max_iter=1000):\n",
    "    # a^t uses h^(t-1)\n",
    "    in_links, out_links = build_in_links(dict(out_links))\n",
    "    nodes = sorted(out_links.keys())\n",
    "\n",
    "    h = {u: 1.0 for u in nodes}\n",
    "    a = {u: 1.0 for u in nodes}\n",
    "\n",
    "    steps = 0\n",
    "    while steps < max_iter:\n",
    "        steps += 1\n",
    "\n",
    "        new_h = {i: sum(a[j] for j in out_links[i]) for i in nodes}\n",
    "        new_a = {i: sum(h[j] for j in in_links[i]) for i in nodes}\n",
    "\n",
    "        new_h = l2_normalize(new_h)\n",
    "        new_a = l2_normalize(new_a)\n",
    "\n",
    "        if max_abs_diff(new_h, h) < tol and max_abs_diff(new_a, a) < tol:\n",
    "            h, a = new_h, new_a\n",
    "            break\n",
    "\n",
    "        h, a = new_h, new_a\n",
    "\n",
    "    return h, a, steps\n",
    "\n",
    "def hits_version2(out_links, tol=1e-9, max_iter=1000):\n",
    "    # a^t uses h^t (updated hubs)\n",
    "    in_links, out_links = build_in_links(dict(out_links))\n",
    "    nodes = sorted(out_links.keys())\n",
    "\n",
    "    h = {u: 1.0 for u in nodes}\n",
    "    a = {u: 1.0 for u in nodes}\n",
    "\n",
    "    steps = 0\n",
    "    while steps < max_iter:\n",
    "        steps += 1\n",
    "\n",
    "        new_h = {i: sum(a[j] for j in out_links[i]) for i in nodes}\n",
    "        new_h = l2_normalize(new_h)\n",
    "\n",
    "        new_a = {i: sum(new_h[j] for j in in_links[i]) for i in nodes}\n",
    "        new_a = l2_normalize(new_a)\n",
    "\n",
    "        if max_abs_diff(new_h, h) < tol and max_abs_diff(new_a, a) < tol:\n",
    "            h, a = new_h, new_a\n",
    "            break\n",
    "\n",
    "        h, a = new_h, new_a\n",
    "\n",
    "    return h, a, steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1302cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chain ===\n",
      "Version 1 steps: 2\n",
      "Version 2 steps: 2\n",
      "V1 hubs: {1: 0.5773502691896258, 2: 0.5773502691896258, 3: 0.5773502691896258, 4: 0.0}\n",
      "V1 auth: {1: 0.0, 2: 0.5773502691896258, 3: 0.5773502691896258, 4: 0.5773502691896258}\n",
      "V2 hubs: {1: 0.5773502691896258, 2: 0.5773502691896258, 3: 0.5773502691896258, 4: 0.0}\n",
      "V2 auth: {1: 0.0, 2: 0.5773502691896258, 3: 0.5773502691896258, 4: 0.5773502691896258}\n",
      "\n",
      "=== Star_out ===\n",
      "Version 1 steps: 2\n",
      "Version 2 steps: 2\n",
      "V1 hubs: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0}\n",
      "V1 auth: {1: 0.0, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5}\n",
      "V2 hubs: {1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0}\n",
      "V2 auth: {1: 0.0, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5}\n",
      "\n",
      "=== Two_hubs_to_two_auths ===\n",
      "Version 1 steps: 2\n",
      "Version 2 steps: 2\n",
      "V1 hubs: {1: 0.7071067811865476, 2: 0.7071067811865476, 3: 0.0, 4: 0.0}\n",
      "V1 auth: {1: 0.0, 2: 0.0, 3: 0.7071067811865476, 4: 0.7071067811865476}\n",
      "V2 hubs: {1: 0.7071067811865476, 2: 0.7071067811865476, 3: 0.0, 4: 0.0}\n",
      "V2 auth: {1: 0.0, 2: 0.0, 3: 0.7071067811865476, 4: 0.7071067811865476}\n"
     ]
    }
   ],
   "source": [
    "graphs = {\n",
    "    \"Chain\": {\n",
    "        1: [2],\n",
    "        2: [3],\n",
    "        3: [4],\n",
    "        4: []\n",
    "    },\n",
    "    \"Star_out\": {\n",
    "        1: [2, 3, 4, 5],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: []\n",
    "    },\n",
    "    \"Two_hubs_to_two_auths\": {\n",
    "        1: [3, 4],\n",
    "        2: [3, 4],\n",
    "        3: [],\n",
    "        4: []\n",
    "    }\n",
    "}\n",
    "\n",
    "TOL = 1e-9\n",
    "\n",
    "for name, g in graphs.items():\n",
    "    h1, a1, s1 = hits_version1(g, tol=TOL)\n",
    "    h2, a2, s2 = hits_version2(g, tol=TOL)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Version 1 steps:\", s1)\n",
    "    print(\"Version 2 steps:\", s2)\n",
    "\n",
    "    print(\"V1 hubs:\", dict(sorted(h1.items())))\n",
    "    print(\"V1 auth:\", dict(sorted(a1.items())))\n",
    "    print(\"V2 hubs:\", dict(sorted(h2.items())))\n",
    "    print(\"V2 auth:\", dict(sorted(a2.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23769cd8",
   "metadata": {},
   "source": [
    "## Problem 4 Discussion\n",
    "\n",
    "**Setup:** Three directed graphs were selected to compare the two HITS versions:\n",
    "1. **Chain** (1→2→3→4): a linear structure with one clear direction of information flow.\n",
    "2. **Star_out** (1→{2,3,4,5}): a single hub pointing to multiple authority nodes.\n",
    "3. **Two_hubs_to_two_auths** ({1,2}→{3,4}): a bipartite structure with two hubs linking to two authorities.\n",
    "\n",
    "Convergence tolerance: 1e-9 with L2 normalization.\n",
    "\n",
    "**Algorithm Difference:**\n",
    "- **Version 1:** Both hub and authority updates use values from the *previous* iteration: h^t from a^(t-1), and a^t from h^(t-1).\n",
    "- **Version 2:** Hubs are updated first using a^(t-1), then authorities use the *already-updated* h^t from the same iteration.\n",
    "\n",
    "**Observations:**\n",
    "- **Converged values:** Both versions converge to the **same** final hub and authority scores for all three graphs. This is expected because the fixed point satisfies h = f(a) and a = g(h) simultaneously, so the update order does not affect the final result.\n",
    "- **Convergence speed:** For these small graphs, both versions converge in very few iterations (often identical step counts), since the structures are simple and the eigenspaces are well-defined. In general, Version 2 can converge in fewer iterations for more complex graphs because the authority update immediately benefits from the freshly computed hub values — analogous to how Gauss-Seidel iteration converges faster than Jacobi iteration in numerical linear algebra.\n",
    "- **Hub/authority interpretation:** In the **Star_out** graph, node 1 is the sole hub (hub score ~1.0) and nodes 2-5 are equal authorities. In **Two_hubs_to_two_auths**, nodes 1 and 2 share the hub role equally, while nodes 3 and 4 share the authority role equally. The **Chain** graph distributes hub scores among nodes 1-3 (which have outgoing edges) and authority scores among nodes 2-4 (which have incoming edges)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
